{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b4716b",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "**Library Used**\n",
    ": nltk  <br>\n",
    "A leading library for working with human language data (text) in Python.\n",
    "\n",
    "**Function Used**\n",
    "\n",
    "    **word_tokenize()**\n",
    "-Splits raw text into individual words and punctuation tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff9fddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Corpus Text:\n",
      "i want english food. sam and i like green vegetables. \n",
      "i like food and english movies. am i going to eat green vegetables again?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Load corpus\n",
    "with open(\"sample_corpus.txt\", \"r\") as f:\n",
    "    corpus_text = f.read().lower()\n",
    "    print(\"ðŸ“„ Corpus Text:\")\n",
    "    print(corpus_text)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(corpus_text)\n",
    "total_tokens = len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d06e7",
   "metadata": {},
   "source": [
    "**FreqDist()**\n",
    "Creates a frequency distribution (like a histogram) for tokens.\n",
    "\n",
    "Example: counts how often each word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d26b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Unigram Table (All Words):\n",
      "          Word  Count  Probability\n",
      "0            .      3     0.107143\n",
      "1            ?      1     0.035714\n",
      "2        again      1     0.035714\n",
      "3           am      1     0.035714\n",
      "4          and      2     0.071429\n",
      "5          eat      1     0.035714\n",
      "6      english      2     0.071429\n",
      "7         food      2     0.071429\n",
      "8        going      1     0.035714\n",
      "9        green      2     0.071429\n",
      "10           i      4     0.142857\n",
      "11        like      2     0.071429\n",
      "12      movies      1     0.035714\n",
      "13         sam      1     0.035714\n",
      "14          to      1     0.035714\n",
      "15  vegetables      2     0.071429\n",
      "16        want      1     0.035714\n"
     ]
    }
   ],
   "source": [
    "# --- Unigram Table ---\n",
    "fdist = FreqDist(tokens)\n",
    "unigram_data = {\n",
    "    \"Word\": [],\n",
    "    \"Count\": [],\n",
    "    \"Probability\": []\n",
    "}\n",
    "\n",
    "for word, count in fdist.items():\n",
    "    unigram_data[\"Word\"].append(word)\n",
    "    unigram_data[\"Count\"].append(count)\n",
    "    unigram_data[\"Probability\"].append(round(count / total_tokens, 6))\n",
    "\n",
    "unigram_df = pd.DataFrame(unigram_data)\n",
    "unigram_df = unigram_df.sort_values(by=\"Word\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Unigram Table (All Words):\")\n",
    "print(unigram_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10069b9f",
   "metadata": {},
   "source": [
    "**ngrams()**\n",
    "Generates n-grams (sequences of n tokens) from a list of words.\n",
    "\n",
    "In this case, youâ€™re using bigrams (2-grams):\n",
    "\n",
    "Example: ['I', 'like', 'apples'] â†’ [('I', 'like'), ('like', 'apples')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea9a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Bigram Table (All Word Pairs):\n",
      "              Bigram  Count  Probability\n",
      "0               . am      1     0.333333\n",
      "1                . i      1     0.333333\n",
      "2              . sam      1     0.333333\n",
      "3            again ?      1     1.000000\n",
      "4               am i      1     1.000000\n",
      "5        and english      1     0.500000\n",
      "6              and i      1     0.500000\n",
      "7          eat green      1     1.000000\n",
      "8       english food      1     0.500000\n",
      "9     english movies      1     0.500000\n",
      "10            food .      1     0.500000\n",
      "11          food and      1     0.500000\n",
      "12          going to      1     1.000000\n",
      "13  green vegetables      2     1.000000\n",
      "14           i going      1     0.250000\n",
      "15            i like      2     0.500000\n",
      "16            i want      1     0.250000\n",
      "17         like food      1     0.500000\n",
      "18        like green      1     0.500000\n",
      "19          movies .      1     1.000000\n",
      "20           sam and      1     1.000000\n",
      "21            to eat      1     1.000000\n",
      "22      vegetables .      1     0.500000\n",
      "23  vegetables again      1     0.500000\n",
      "24      want english      1     1.000000\n"
     ]
    }
   ],
   "source": [
    "# --- Bigram Table ---\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigram_fdist = FreqDist(bigrams)\n",
    "prev_word_counts = FreqDist(w1 for (w1, w2) in bigrams)\n",
    "\n",
    "bigram_data = {\n",
    "    \"Bigram\": [],\n",
    "    \"Count\": [],\n",
    "    \"Probability\": []\n",
    "}\n",
    "\n",
    "for bg, count in bigram_fdist.items():\n",
    "    w1, w2 = bg\n",
    "    prob = count / prev_word_counts[w1] if prev_word_counts[w1] > 0 else 0\n",
    "    bigram_data[\"Bigram\"].append(f\"{w1} {w2}\")\n",
    "    bigram_data[\"Count\"].append(count)\n",
    "    bigram_data[\"Probability\"].append(round(prob, 6))\n",
    "\n",
    "bigram_df = pd.DataFrame(bigram_data)\n",
    "bigram_df = bigram_df.sort_values(by=\"Bigram\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Bigram Table (All Word Pairs):\")\n",
    "print(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33b3ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity on test sequence: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# --- Bigram Probability Helper ---\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_fdist[(w1, w2)] / prev_word_counts[w1] if prev_word_counts[w1] > 0 else 0\n",
    "\n",
    "def perplexity(test_bigrams):\n",
    "        N = len(test_bigrams)\n",
    "        log_prob_sum = 0\n",
    "        for bg in test_bigrams:\n",
    "            p = bigram_prob(bg[0], bg[1])\n",
    "            if p > 0:\n",
    "                log_prob_sum += math.log2(p)\n",
    "            else:\n",
    "                return float('inf')\n",
    "        return 2 ** (-log_prob_sum / N)\n",
    "test_sentence = input(\"Enter the sentence you want to check the perplexity\")\n",
    "test_tokens = word_tokenize(test_sentence.lower())\n",
    "test_bigrams = list(ngrams(test_tokens, 2))\n",
    "print(f\"\\nPerplexity on test sequence: {perplexity(test_bigrams):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356d154",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "### Functions Used\n",
    "\n",
    "**1) pos_tag()**\n",
    ": Assigns Part-of-Speech (POS) tags to each token using a pretrained model.\n",
    "\n",
    "Example: ['I', 'like', 'food'] â†’ [('I', 'PRP'), ('like', 'VBP'), ('food', 'NN')]\n",
    "\n",
    "**2) brown.tagged_sents()**\n",
    ": Loads tagged sentences from the Brown corpus.\n",
    "\n",
    "categories='news' restricts to the \"news\" category.\n",
    "\n",
    "Used to train your HMM.\n",
    "\n",
    "**3)ConditionalFreqDist()**\n",
    ": A frequency distribution conditioned on a key.\n",
    "\n",
    "Used here to:\n",
    "\n",
    "Count word frequencies given a tag (for emissions)\n",
    "\n",
    "Count tag frequencies given a previous tag (for transitions)\n",
    "\n",
    "**4)LidstoneProbDist()**\n",
    ": Applies Lidstone smoothing (a generalization of Laplace smoothing).\n",
    "\n",
    "Prevents zero probabilities in emission/transition models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- POS Tagging ---\n",
      "[('i', 'NN'), ('want', 'VBP'), ('to', 'TO')]\n",
      "\n",
      "--- HMM Model Trained ---\n",
      "Sample emission P('food' | 'NN') = 0.000031\n",
      "Sample transition P('VB' | '<s>') = 0.000099\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import ConditionalFreqDist, LidstoneProbDist\n",
    "\n",
    "# a) Simple POS Tagging\n",
    "sentence = input(\"Enter the sentence:\")\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "print(\"\\n--- POS Tagging ---\")\n",
    "print(tags)\n",
    "\n",
    "# b) HMM model training (transitions and emissions)\n",
    "tagged_sents = brown.tagged_sents(categories='news')[:1000]\n",
    "cfd_tag_word = ConditionalFreqDist()\n",
    "cfd_tag_tag = ConditionalFreqDist()\n",
    "\n",
    "for sent in tagged_sents:\n",
    "    prev_tag = \"<s>\"\n",
    "    for word, tag in sent:\n",
    "        cfd_tag_word[tag][word.lower()] += 1\n",
    "        cfd_tag_tag[prev_tag][tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "emissions = {\n",
    "    tag: LidstoneProbDist(cfd_tag_word[tag], 0.1, bins=len(cfd_tag_word[tag]))\n",
    "    for tag in cfd_tag_word\n",
    "}\n",
    "\n",
    "transitions = {\n",
    "    tag: LidstoneProbDist(cfd_tag_tag[tag], 0.1, bins=len(cfd_tag_tag[tag]))\n",
    "    for tag in cfd_tag_tag\n",
    "}\n",
    "\n",
    "print(\"\\n--- HMM Model Trained ---\")\n",
    "print(f\"Sample emission P('food' | 'NN') = {emissions['NN'].prob('food'):.6f}\")\n",
    "print(f\"Sample transition P('VB' | '<s>') = {transitions['<s>'].prob('VB'):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f2067",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Module Used\n",
    "spacy â€“ Industrial-Strength NLP Library\n",
    "spaCy is a powerful and fast NLP library used for:\n",
    "\n",
    "i)Tokenization\n",
    "\n",
    "ii)Named Entity Recognition (NER)\n",
    "\n",
    "iii)POS tagging\n",
    "\n",
    "iv)Dependency parsing\n",
    "\n",
    "## Functions Used\n",
    "1)**spacy.load(\"en_core_web_sm\")**\n",
    ": Loads a pre-trained small English pipeline\n",
    "\n",
    "Includes tokenizer, POS tagger, NER, etc.\n",
    "\n",
    "\n",
    "\n",
    "2)**nlp(text)**\n",
    ": Passes the input text through the entire pipeline\n",
    "\n",
    "Returns a Doc object with tokens, entities, etc.\n",
    "\n",
    "3)**token.ent_iob_**\n",
    "IOB tag: used for NER labeling\n",
    "\n",
    "Values:\n",
    "\n",
    "'B': Beginning of named entity\n",
    "\n",
    "'I': Inside a named entity\n",
    "\n",
    "'O': Outside any named entity\n",
    "\n",
    "4)**token.ent_type_**\n",
    "The entity type like:\n",
    "\n",
    "PERSON, ORG, GPE (geo-political entity), DATE, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13c63063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NER Output (IOB format) ---\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "text = input(\"Enter text:\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print IOB format output\n",
    "print(\"\\n--- NER Output (IOB format) ---\")\n",
    "for token in doc:\n",
    "    iob = token.ent_iob_\n",
    "    label = token.ent_type_ if token.ent_type_ else \"O\"\n",
    "    print(f\"{token.text}\\t{iob}-{label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
